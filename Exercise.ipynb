{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise X: Gradient analysis of a neural network\n",
    "\n",
    "You have already learnt to train a neural network and to evaluate its perfomance. In this exercise, you will analyze a neural network more closely. The output of the neural network is a function of the input quantities and Tensorflow enables you to derive gradients of it. The training algorithm uses gradients with respect to the NN parameters for given inputs defined by the training set. The other way round, we are going to derive gradients with respect to the input quantities for the given NN parameters determined by the training in order to get an idea how the NN output depends on the respective inputs and what their relevance is. It can also be understood as evaluating the leading orders of a Taylor Expansion of the NN output around the data points.\n",
    "\n",
    "As you know already, you should usually get a basic overview of the input data before you apply multivariate analysis techniques. This is often limited by the complexity of the given data and advanced analysis techniques like the gradient analysis may help you to complete the picture. In this example however, the input data is of rather simple structure and we won't take a look at the beginning in order to pretend that it is more complex. We will make plots later on in order to verify the conclusions that we made from the gradient analysis.\n",
    "\n",
    "The dataset contains two event classes (signal and background) that we want to optimally classify. There are three observables that can be use for this purpose.\n",
    "\n",
    "To guide your own implementation of studies of the data-set, a code basis is provided in this jupyter notebook. Inspect it and use it as a starting point for your own work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages needed for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing usual python packages\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "# Importing ML related packages. \n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the tensorflow version we are using. Should be 2.2.0\n",
    "print(tensorflow.__file__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset. It consists of 10000 signal and background events respectively for the training. The same amount of events is available as test set. The first three columns contain the observables. The forth column contains the truth label (see mapping below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.loadtxt('events.data')\n",
    "np.random.shuffle(data) # input data are sorted by class and Keras takes the last X% of the input data as validation set\n",
    "data_variables = np.array(data[:, :3])\n",
    "labels = data[:, 3]\n",
    "bckgrd = labels==0\n",
    "signal = labels==1\n",
    "training_set = labels<2\n",
    "bckgrd_test = labels==2\n",
    "signal_test = labels==3\n",
    "test_set = labels>=2\n",
    "\n",
    ">>>Mention one-line sklearn function https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise X.1: Training and retrieval of gradients (obligatory)\n",
    "For this relatively simple training task we use a small NN as defined in the following. Run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Select training data\n",
    "    input_variables = data_variables[training_set]\n",
    "    input_labels = labels[training_set]\n",
    "\n",
    "    # Set up model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation = 'tanh', input_dim = 3))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # Define loss function, optimizer algorithm and validation metrics\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['acc'])\n",
    "\n",
    "    # Print summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(input_variables, input_labels, batch_size=1024, epochs=50, validation_split=0.25)\n",
    "\n",
    "    # Get training and validation loss/accuracy values from history\n",
    "    loss_training = history.history['loss']\n",
    "    loss_validation = history.history['val_loss']\n",
    "    accuracy_training = history.history['acc']\n",
    "    accuracy_validation = history.history['val_acc']\n",
    "\n",
    "    # Plot the training and validation loss/accuracy vs the number of epochs\n",
    "    plt.plot(accuracy_training)\n",
    "    plt.plot(accuracy_validation)\n",
    "    plt.savefig('loss_vs_epochs.png')   \n",
    "    \n",
    "    # Save model to file\n",
    "    model.save('model.hd5')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the NN to the test data and plot a histogram of the resulting distributions in order to get a visual impression of the output distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(events, modelfile='model.hd5'):\n",
    "    # Load trained keras model\n",
    "    model = load_model(modelfile)\n",
    "    \n",
    "    # Pass events through model\n",
    "    predictions = model.predict(events)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows you how the gradient can be retrieved with a tensorflow gradient tape. It tells tensorflow with respect to which parameters of the NN tensor the gradient shall be calculated (The input observables in this case).\n",
    "With the given loop, the gradient will be evaluated at each data point of the test set and the mean result will be printed at the end. This tells you how the NN output responds on average to variations of the input observables. Run the code and get the mean gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(sample, model):\n",
    "    with tensorflow.GradientTape() as tape: #https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients\n",
    "        inputs = tensorflow.Variable([sample])\n",
    "        tape.watch(inputs)\n",
    "        output = model(inputs)\n",
    "    g = tape.gradient(output, inputs)\n",
    "    grads = g.numpy()[0]\n",
    "    return grads # hint: you can return multiple objects at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.hd5')\n",
    "mean_grads = [0.0, 0.0, 0.0]\n",
    "n_evts = len(data_variables[test_set])\n",
    "fraction = 10 # run only every nth event for saving time\n",
    "for n, sample in enumerate(data_variables[test_set]):\n",
    "    if n%fraction!=0:\n",
    "        continue\n",
    "    grads = get_gradients(sample, model)\n",
    "    for i in range(len(mean_grads)):\n",
    "        mean_grads[i] += grads[i]/n_evts*fraction\n",
    "print(mean_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean gradient contains one component for each input observable.\n",
    "- What does the result tell you?\n",
    "- What is the drawback of this calculated mean gradient? How can it be improved? Only a small adaptation of the given code is necessary. Apply and rerun it.\n",
    "- What additional information can you get from the new result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate higher derivatives with tensorflow. In order to go to the second order, you can encapsulate the present gradient tape with a second one and retrieve the jacobian matrix via `jacobian = tape2.jacobian(g, inputs).numpy()`. Note that the resulting numpy array will have effectively two dimensions but like for the gradient you need to call the zeroth component in between `jacobian[0][i][0][j]`. Calculate the second derivatives given by the diagonal elements. What do they tell you and how does that relate to the observation you made with the first order gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to verify your assumptions about the data distributions. Let's create a 3x3 matrix of plots of the three observables (marginal distributions on the diagonal, 2D scatter plots on the off-diagonal elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvar = 3\n",
    "f, axarr = plt.subplots(nvar, nvar, figsize=[12.8, 9.6])\n",
    "nbins = 20\n",
    "for i in range(nvar):\n",
    "    for j in range(nvar):\n",
    "        if i==j:\n",
    "            axarr[i,j].hist(data[signal_test,i], histtype=u'step', bins=11, range=(-1,4), label=\"signal\")\n",
    "            axarr[i,j].hist(data[bckgrd_test,i], histtype=u'step', bins=11, range=(-1,4), label=\"background\")\n",
    "            axarr[i,j].legend()\n",
    "            axarr[i,j].set_xlabel(\"Observable %i\"%(i+1))\n",
    "            axarr[i,j].set_ylabel(\"# events\")\n",
    "        else:\n",
    "            axarr[i,j].scatter(data[signal_test,i],data[signal_test,j], s=1, alpha=0.3, label=\"signal\")\n",
    "            axarr[i,j].scatter(data[bckgrd_test,i],data[bckgrd_test,j], s=1, alpha=0.3, label=\"background\")\n",
    "            axarr[i,j].legend()\n",
    "            axarr[i,j].set_xlabel(\"Observable %i\"%(i+1))\n",
    "            axarr[i,j].set_ylabel(\"Observable %i\"%(j+1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise X.2: ROC curves (obligatory)\n",
    "In this part of the exercise, you will generate ROC curves in order to visualize the separation power of the single observables and the NN output. Finally, compare it to the information you retrieved with the gradient analysis.\n",
    "\n",
    "The following function already provides the functionality to transform two histrograms (signal and background) handed over as a list of bin contents into ROC plots. Recall what it does and what a ROC curve shows.\n",
    "\n",
    "In the second code block, define the histograms to be fed into the function for each of the observables and the NN output generating altogether four ROC curves. One example is already given. In one case, you will need to apply a simple transformation to the observable in order to get a sensible results. You can apply the necessary operations like `abs()+-*/` directly to the numpy array that holds the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(y_sig, y_bkg, label):\n",
    "    # Choose the direction of the discriminating varible such that AUC is > 50%\n",
    "    mean = 0.0\n",
    "    for i in range(len(y_sig)):\n",
    "        mean += i * (y_sig[i] - y_bkg[i])\n",
    "    if mean > 0.0:\n",
    "        y_sig = y_sig[::-1] # reverse numpy array\n",
    "        y_bkg = y_bkg[::-1] # reverse numpy array\n",
    "    \n",
    "    # Transform bin contents into lists off efficiencies\n",
    "    sigeff = [0.0]\n",
    "    bkgeff = [0.0]\n",
    "    for y, eff in [[y_sig, sigeff], [y_bkg, bkgeff]]:\n",
    "        for entry in y:\n",
    "            eff.append(entry + eff[-1])\n",
    "        for i, entry in enumerate(eff):\n",
    "            eff[i] = entry / eff[-1]\n",
    "    bkgrej=[]\n",
    "    for entry in bkgeff:\n",
    "        bkgrej.append(1.0 - entry)\n",
    "    \n",
    "    plt.plot(sigeff, bkgrej, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1sig, x1sig, p1sig = plt.hist(data[signal_test,0], histtype=u'step', bins=40, range=(-2,5), label=\"signal\")\n",
    "y1bkg, x1bkg, p1bkg = plt.hist(data[bckgrd_test,0], histtype=u'step', bins=40, range=(-2,5), label=\"background\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Observable 1\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.show()\n",
    "# TODO: add further histograms\n",
    "\n",
    "plot_ROC(y1sig, y1bkg, \"Observable 1\")\n",
    "# TODO: add further ROC curves\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise X.3: Visualization of NN function and derivatives (obligatory)\n",
    "You have probably found out that one observable does not contribute any separation power. Exclude it and repeat the training with only the two useful observables such that we can easily visualize the NN function and its dervatives as 2D heat maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reduced():\n",
    "    # Select labelled data\n",
    "    input_variables_reduced = data_variables_reduced[training_set]\n",
    "    input_labels = labels[training_set]\n",
    "\n",
    "    # Set up model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation = 'tanh', input_dim = 2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # Define loss function, optimizer algorithm and validation metrics\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['acc'])\n",
    "\n",
    "    # Print summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(input_variables_reduced, input_labels, batch_size=1024, epochs=50, validation_split=0.25)\n",
    "\n",
    "    # Get training and validation loss/accuracy values from history\n",
    "    loss_training = history.history['loss']\n",
    "    loss_validation = history.history['val_loss']\n",
    "    accuracy_training = history.history['acc']\n",
    "    accuracy_validation = history.history['val_acc']\n",
    "\n",
    "    # Plot the training and validation loss/accuracy vs the number of epochs\n",
    "    plt.plot(accuracy_training)\n",
    "    plt.plot(accuracy_validation)\n",
    "    plt.savefig('loss_vs_epochs.png')   \n",
    "    \n",
    "    # Save model to file\n",
    "    model.save('model_reduced.hd5')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variables_reduced = # TODO: define your reduced variable set\n",
    "train_reduced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code snippets. They will produce heat maps of the NN output, first order gradients and the jacobian matrix. Inpect the results and understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_reduced.hd5')\n",
    "\n",
    "# variables to store grids of quantities of interest\n",
    "f = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "dd1 = []\n",
    "dd2 = []\n",
    "d1d2 = []\n",
    "\n",
    "# loop through grid\n",
    "for i in range(40):\n",
    "    # store lines as an intermediate step\n",
    "    lf = []\n",
    "    ld1 = []\n",
    "    ld2 = []\n",
    "    ldd1 = []\n",
    "    ldd2 = []\n",
    "    ld1d2 = []\n",
    "    for j in range(40):\n",
    "        point = np.array([5.-i/40.*7., j/40.*7.-2.]) #decreasing in i because of image coordinate system of imshow\n",
    "        grads, jacobian = get_gradients(point, model) # TODO: check that this is consistent with your implementation of get_gradients\n",
    "        lf.append(output.numpy()[0])\n",
    "        ld1.append(grads[0])\n",
    "        ld2.append(grads[1])\n",
    "        ldd1.append(jacobian[0][0][0])\n",
    "        ldd2.append(jacobian[1][0][1])\n",
    "        ld1d2.append(jacobian[1][0][0])\n",
    "    f.append(lf)\n",
    "    d1.append(ld1)\n",
    "    d2.append(ld2)\n",
    "    dd1.append(ldd1)\n",
    "    dd2.append(ldd2)\n",
    "    d1d2.append(ld1d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot NN output function\n",
    "plt.imshow(f, extent=[-2,5,-2,5], cmap='Blues', vmin=0.0, vmax=1.0)\n",
    "plt.xlabel(\"Observable 2\")\n",
    "plt.ylabel(\"Observable 1\")\n",
    "plt.colorbar(label=\"NN output\")\n",
    "plt.show()\n",
    "\n",
    "# plot first derivatives\n",
    "fig, axarr = plt.subplots(1, 2, figsize=[10, 5])\n",
    "pcm = axarr[0].imshow(d1, extent=[-2,5,-2,5], cmap='RdBu', vmin=-1.0, vmax=1.0)\n",
    "axarr[0].set_xlabel(\"Observable 2\")\n",
    "axarr[0].set_ylabel(\"Observable 1\")\n",
    "fig.colorbar(pcm, ax=axarr[0], label=\"d(NN output)/d(Obs.1)\")\n",
    "axarr[1].imshow(d2, extent=[-2,5,-2,5], cmap='RdBu', vmin=-1.0, vmax=1.0)\n",
    "axarr[1].set_xlabel(\"Observable 2\")\n",
    "axarr[1].set_ylabel(\"Observable 1\")\n",
    "fig.colorbar(pcm, ax=axarr[1], label=\"d(NN output)/d(Obs.2)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot second derivatives\n",
    "fig, axarr = plt.subplots(2, 2, figsize=[10, 10])\n",
    "pcm2 = axarr[0,0].imshow(dd1, extent=[-2,5,-2,5], cmap='RdBu', vmin=-1.0, vmax=1.0)\n",
    "axarr[0,0].set_xlabel(\"Observable 2\")\n",
    "axarr[0,0].set_ylabel(\"Observable 1\")\n",
    "fig.colorbar(pcm2, ax=axarr[0,0], label=\"d^2(NN output)/d(Obs.1)^2\")\n",
    "axarr[0,1].imshow(d1d2, extent=[-2,5,-2,5], cmap='RdBu', vmin=-1.0, vmax=1.0)\n",
    "axarr[0,1].set_xlabel(\"Observable 2\")\n",
    "axarr[0,1].set_ylabel(\"Observable 1\")\n",
    "fig.colorbar(pcm2, ax=axarr[0,1], label=\"d^2(NN output)/d(Obs.1)d(Obs.2)\")\n",
    "axarr[1,1].imshow(dd2, extent=[-2,5,-2,5], cmap='RdBu', vmin=-1.0, vmax=1.0)\n",
    "axarr[1,1].set_xlabel(\"Observable 2\")\n",
    "axarr[1,1].set_ylabel(\"Observable 1\")\n",
    "fig.colorbar(pcm2, ax=axarr[1,1], label=\"d^2(NN output)/d(Obs.2)^2\")\n",
    "axarr[1,0].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, think about what impacts the gradient based key figures. In case of effects that deteriorate the interpretability, can you think of measures to mitigate these effects?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
